paper 1:
Multi-Vision Multi-Prompt for Few-Shot Learning in Vision-Language Model:
  Title: Multi-Vision Multi-Prompt for Few-Shot Learning in Vision-Language Model
  Problem Statement: In vision-language models like CLIP, prompt learning is used to adapt to specific tasks in few-shot learning. However, existing methods often rely on a single prompt, which may not accurately distinguish between different categories, especially when a category has multiple features and contextual connections. This limitation affects the model's accuracy and efficiency.
  Motivation: Existing methods such as meta-learning or image augmentation can improve few-shot learning performance but often at the cost of increased computational complexity and reduced accuracy. Single-prompt methods fail to capture the diverse features and contextual nuances of different categories. The proposed Multi-Vision Multi-Prompt (MVMP) method aims to address these issues by using multiple prompts at different stages of the training process and averaging the predictions, thereby enhancing the model's performance without increasing the number of model parameters.
  Proposed Method: MVMP employs multiple prompts at different stages of the training process and averages the predictions to improve accuracy. It also introduces a mixed self-augmentation framework and text distillation to enhance the model's performance. The method involves three key techniques: (a) Multi-prompt: Using multiple prompts at different training stages and performing information fusion through average weighting. (b) Mixed self-augmentation: Creating new virtual images by replacing region pixels with pixels from another image region to enhance image diversity. (c) Text distillation: Using multiple fixed prompts to acquire diverse text features, thereby improving the alignment between image and text.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: Use 11 different visual categorization datasets including ImageNet, OxfordPets, StanfordCars, Caltech101, DTD, EuroSAT, FGVCAircraft, Flowers102, Food101, SUN397, and UCF101. These datasets cover a wide range of tasks from large-scale image classification to more specialized and fine-grained tasks.
    Step 2: Construct Prompts: For the proposed method, use multiple prompts stored in a prompt memory bank. During training, update the memory bank with prompts from different stages and apply Gaussian weights to high-layer prompts. For text distillation, use 10 pre-defined manual prompts to distill frozen CLIP text features.
    Step 3: Select Models: Use the pre-trained CLIP model with VIT-B/16 as the image encoder. The text encoder and image encoder of CLIP are frozen, and only the embedded text and image prompts are trained.
    Step 4: Training Process: Train the model for 50 epochs with a batch size of 16. Use a learning rate of 0.002 and set weights of the loss function as λ1=2, λ2=15, λ3=5. For each training epoch, update the prompt memory bank and apply Gaussian weights to high-layer prompts. Use mixed self-augmentation to create new virtual samples and apply consistency loss to stabilize the model.
    Step 5: Inference Process: During inference, sample prompts from the memory bank and use them to obtain text feature diversity. Calculate the prediction distribution for each image by averaging the predictions from multiple prompts and the fixed text features.
    Step 6: Get Results: Evaluate the model on the test sets of the 11 datasets for few-shot learning tasks with 1, 2, 4, 8, and 16 shots. Compare the performance of MVMP with state-of-the-art prompt learning methods and traditional few-shot learning data augmentation methods.
    Step 7: Analyze Results: Compare the accuracy of MVMP with other methods on each dataset. Analyze the improvements in performance, especially in scenarios with minimal training samples. Evaluate the generalization performance of MVMP in cross-dataset experiments and base-to-new experiments.


paper 2:
Thought Propagation:
  Title: THOUGHT PROPAGATION: AN ANALOGICAL APPROACH TO COMPLEX REASONING WITH LARGE LANGUAGE MODELS
  Problem Statement: Large Language Models (LLMs) have achieved remarkable success in reasoning tasks with the development of prompting methods. However, existing prompting approaches cannot reuse insights from solving similar problems and suffer from accumulated errors in multi-step reasoning, as they prompt LLMs to reason from scratch.
  Motivation: Existing methods prompt LLMs to reason from scratch, which leads to two main issues: 1) They cannot reuse insights from solving similar problems, which could ease the difficulty of solving complex problems and develop new solutions. 2) They are sensitive to errors made in intermediate stages during multi-step reasoning, leading to accumulated errors and invalid outcomes. The inspiration behind the new proposed method, Thought Propagation (TP), is to explore analogous problems and leverage their solutions to enhance the complex reasoning ability of LLMs. By propagating insights from solving previous analogous problems, TP aims to inspire new problem-solving and address the limitations of reasoning from scratch.
  Proposed Method: Thought Propagation (TP) involves three main modules: LLM Propose, LLM Solve, and LLM Aggregate. TP first prompts LLMs to propose and solve a set of analogous problems related to the input one. Then, TP reuses the results of analogous problems to directly yield a new solution or derive a knowledge-intensive plan for execution to amend the initial solution obtained from scratch. TP is compatible with existing prompting approaches, allowing plug-and-play generalization and enhancement in a wide range of tasks without much labor in task-specific prompt engineering.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: We use three challenging tasks to evaluate the proposed method: Shortest-Path Reasoning, Creative Writing, and LLM-Agent Planning. For Shortest-Path Reasoning, we generate 100 non-trivial shortest-path problems. For Creative Writing, we use a dataset with 100 writing problems. For LLM-Agent Planning, we use the ALFWorld game suite with 134 environments.
    Step 2: Construct Prompts:
      Shortest-Path Reasoning:
        LLM Propose: The undirected graph is represented as a node set, an edge set. Given an input node, find its neighborhood nodes and save them in a list. Input: Node set: [0, 1, 2, 3, 4] Edge set: [[0, 3], [1, 4], [2, 4], [3, 4]] Input Node: 4 Answer: The neighborhood node list of the input node is [1, 2, 3].
        LLM Solve: Find the shortest path from a source node to a target node in an undirected graph. The undirected graph is represented as a node set, an edge set, and an edge distance set. Input: Node set: [0, 1, 2, 3, 4] Edge set: [[0, 3], [1, 4], [2, 4], [3, 4]] Edge distance set: [2, 3, 5, 3] Source Node: 0 Target Node: 4 Answer: The shortest path from the source node to the target node is [0, 3, 4]. The shortest distance is 5.
        LLM Aggregate: The undirected graph is represented as a node set, an edge set and an edge distance set. The edges in the edge set are reversible. We have hints of one or several intermediate paths from the source node to some intermediate nodes. Please use these hints to find the shortest path from the source node the the target node. Input: Node set: [0, 1, 2, 3, 4] Edge set: [[0, 3], [1, 4], [2, 4], [3, 4]] Edge distance set: [2, 3, 5, 3] The hints are: The shortest path from the source node 0 to the intermediate node 3 is [0, 3]. The shortest distance is 2. The shortest path from the source node 0 to the intermediate node 1 is [0, 3, 4, 1]. The shortest distance is 8. The shortest path from the source node 0 to the intermediate node 2 is [0, 3, 4, 2]. The shortest distance is 10. Use the above hint to find the shortest path from the source node 0 to the target node 4. Answer: Using the above hints, the shortest path from the source node 0 to the target node 4 is [0, 3, 4]. The shortest distance is 5.
      Creative Writing:
        LLM Propose: Please rephrase the input sentences but do not change their order or meaning. The input sentences are: {input} Your output should be in the following format: Output: Your sentences here.
        LLM Solve: Make a writing plan for a coherent passage of 4 short paragraphs. The end sentence of each paragraph must be: {input} The plan contains a one-sentence description in each paragraph. Your output should be in the following format: Plan: Your plan here.
        LLM Aggregate: Given several writing plans, decide which writing plan is the most promising. Analyze each writing plan in detail, then conclude in the last line 'The best choice is s', where s is the integer id of the choice.
      LLM-Agent Planning:
        LLM Propose: Evaluate the similarity score between these two cases. The similarity score should be an integer between 0 and 10. 0 indicates the least similarity and 10 indicates the most similarity. Case 1: {current task description } Case 2: {proposed task description } The output format should be: The similarity score is:
        LLM Solve: You will be given a successful case where you successfully complete the task. Then you will be given a failure case where you fail to complete the task. Do not summarize these two cases, but rather use the successful case to think about the strategy and path you took to attempt to complete the task in the failure case. Devise a concise, new plan of action that accounts for your mistake with reference to specific actions that you should have taken. For example, if you tried A and B but forgot C, then devise a plan to achieve C with environment-specific actions. You will need this later to solve the failure case. Give your plan after 'Plan'. Success Case: {success case } Failure Case: {failure case } Plan:
        LLM Aggregate: You once fail to accomplish the following task. The task is: {task} Here are several plans to accomplish the task: {several plans } Output the most promising plan to accomplish the task, but do not output any irrelevant messages. Your answer goes after Plan: Plan:
    Step 3: Select Models: We use three main strong base LLMs across all tasks: PaLM-2 (Bison), GPT-3.5, and GPT-4.
    Step 4: Get Results: Get answer predictions from the models on these datasets with both the baselines and proposed method.
    Step 5: Analyze Results: Compare whether the new method improves the performance of LLMs in these tasks.



paper 1 meta review:
 All reviewers are quite negative on this paper. They have raised a series of concerns. Many parts in the work are not clearly presented. Some important related and recent works are missing. The experimental comparisons are also not fair. No rebuttal is provided by the authors. Thus, the AC recommends rejection for this paper.
Justification for why not higher score: There are many weaknesses in this paper, in terms of clarity, comparisons and experiments.
Justification for why not lower score: N/A

paper 1 score:  2.5



paper 2 meta review:
 This paper proposes a new approach called Thought Propagation (TP) for leveraging LLMs for solving reasoning tasks. TP takes three sequential steps: (1) prompting LLMs to first generate multiple problems similar to the target problem; (2) prompting LLMs for solving the similar, generated problems and then (3)  using LLMs to combine the generated answers to create a final answer to the target problem.

TP is shown to outperform state-of-the-art methods (Tree of Thoughts and Chain of Thoughts) on most tested benchmarks. However, the downside raised by some reviewers (`39VY`, `yu29`) and that the authors agree to is that TP is substantially slower than ToT and CoT.

The authors did a satisfactory job in answering the rebuttal questions from all three reviewers and requests for additional results.

I vote for the paper to be `Accepted` given its extensive experiments, large quantitative improvement (especially in 0-shot and lower-shot settings) compared to previous methods.
However, I'd request the authors to include the computation details/disadvantages in the Appendix and refer to it from a Limitation discussion in the main text.
Justification for why not higher score: Despite being more accurate, TP is also substantially more computationally expensive compared to prior methods.
Justification for why not lower score: The paper has extensive experiments showing its superior accuracy compared to baselines.

paper 2 score:  6.7


accepted paper:  2




paper 1:
Diagnosing Transformers:
  Title: Diagnosing Transformers: Illuminating Feature Spaces for Clinical Decision-Making
  Problem Statement: Pre-trained transformers are often fine-tuned to aid clinical decision-making using limited clinical notes. Model interpretability is crucial in high-stakes domains like medicine to establish trust and ensure safety, which requires human engagement.
  Motivation: Existing methods for fine-tuning transformers in clinical settings often lack interpretability, making it difficult to trust and understand their decisions. This is particularly problematic in medicine, where understanding model behavior is essential for safety and efficacy. The proposed method, SUFO, aims to enhance interpretability by systematically analyzing and visualizing the feature spaces of fine-tuned transformers, addressing key questions about model trust and interpretability.
  Proposed Method: SUFO is a systematic framework that enhances interpretability of fine-tuned transformer feature spaces. It utilizes a range of analytic and visualization techniques, including Supervised probing, Unsupervised similarity analysis, Feature dynamics, and Outlier analysis. These techniques help to evaluate model suitability for a task, understand feature space evolution during fine-tuning, and interpret fine-tuned features and failure modes.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: We use a real-world pathology report dataset and the MedNLI dataset. The pathology report dataset includes 2907 structured reports with four pathologic data elements: primary Gleason grade, secondary Gleason grade, margin status for tumor, and seminal vesicle invasion. MedNLI is used to validate findings and includes clinical hypotheses labeled with contradiction, entailment, and neutral.
    Step 2: Select Models: We evaluate five 110M-sized pre-trained transformer models: BERT and TNLR (general-domain), BioBERT and Clinical BioBERT (mixed-domain), and PubMedBERT (domain-specific).
    Step 3: Fine-Tune Models: Fine-tune each model on the pathology report dataset and MedNLI. Use consistent hyperparameters for all models: AdamW optimizer, learning rate of 7.6×10−6, weight decay of 0.01, epsilon of 1×10−8, linear learning rate schedule with 0.2 warm-up ratio, batch size of 8, and a maximum of 25 epochs for pathology reports. For MedNLI, use a per-layer learning rate decay schedule, starting learning rate of 1×10−4, decay factor of 0.8, weight decay of 0, epsilon of 1×10−6, warm-up ratio of 0.1, batch size of 32, and a maximum of 10 epochs.
    Step 4: Supervised Probing: Evaluate the pre-trained features by freezing the pre-trained weights and only training the last linear layer. Compare the performance of the models on the pathology report dataset using macro F1 scores and on MedNLI using accuracy.
    Step 5: Unsupervised Similarity Analysis: Conduct Representational Similarity Analysis (RSA) to measure the similarity of feature spaces between pre-trained and fine-tuned models. Use a set of 1000 random reports as control stimuli and calculate pairwise similarity matrices using Euclidean distance. Compute the Pearson correlation between the flattened upper triangular sections of the two pairwise similarity matrices.
    Step 6: Feature Dynamics: Use PCA to investigate feature dynamics across layers and time during fine-tuning. Extract activations of corresponding encoder layers at the classification token across 25 checkpoints. Analyze the structure and evolution of feature disambiguation during fine-tuning.
    Step 7: Outlier Analysis: Analyze the principle components of features in the final layer classification token of the fine-tuned models. Identify outlier reports by constructing clusters in the two-dimensional singular subspace of the feature space. Solicit expert evaluation to determine the causes of outlier behavior and compare the models based on their sensitivity to different types of outliers.
    Step 8: Analyze Results: Compare the performance of the models on the pathology report dataset and MedNLI. Evaluate the robustness of mixed-domain models under class imbalance, the effect of in-domain pre-training on feature disambiguation, and the ability to identify missing medical information. Validate findings through expert evaluation of outlier reports.


paper 2:
Learning Customized Human Preferences:
  Title: Everyone Deserves A Reward: Learning Customized Human Preferences
  Problem Statement: The paper addresses the problem of aligning large language models (LLMs) with diverse human preferences. Current methods only consider a general reward model, which is insufficient for customized or personalized application scenarios. The real world is pluralistic, leading to diversified human preferences based on different religions, politics, cultures, etc., and each individual can have unique preferences on various topics.
  Motivation: Existing methods neglect the diversity of human preferences and only consider a general reward model, which is below satisfaction for customized or personalized application scenarios. The inspiration behind the new proposed method is to explore customized preference learning by collecting a domain-specific preference (DSP) dataset and proposing a three-stage customized RM learning scheme. The proposed method would work better than existing baselines by better preserving the general preferring ability while training the customized RMs, especially through general preference enrichment and customized preference imitation learning.
  Proposed Method: The proposed method involves a three-stage training scheme for customized reward models (RMs): 1) Base LM Training: Train a transformer with the language modeling loss as the RM base. 2) General RM Fine-tuning (GRFT): Add a reward head on the top of the base LM, then fine-tune RM with general preference data. 3) Customized RM Fine-tuning (CRFT): Use a trained general RM and continue fine-tuning it on customized human preferences. The method also involves collecting a domain-specific preference (DSP) dataset using ChatGPT to simulate domain-specific human preferences.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: Collect the following datasets: 1) Domain-Specific Preference (DSP) dataset with responses from four domains: Academy, Business, Entertainment, and Literature&Art. 2) Helpful&Harmless (H&H) dataset with 46K and 45K comparison data pairs, respectively. 3) WebGPT dataset with 19.6K samples. 4) GPT-4-LLM dataset with 52K unique instructions from the Alpaca training set.
    Step 2: Construct Prompts: For the DSP dataset, use crafted system prompts to let ChatGPT act as an experienced practitioner in each domain and answer each user query as a domain-preferred response. For example, for the Academy domain, the system prompt could be: 'You are an experienced academic researcher. Please provide a detailed and analytical response to the following query.'
    Step 3: Select Models: Use LLaMA-7B and Alpaca-7B as the base models for RM training. Add a reward head on the top of the transformer blocks, which takes the pooled last hidden states as inputs and outputs real-valued reward scores.
    Step 4: Train Models: 1) Base LM Training: Train the base models (LLaMA-7B and Alpaca-7B) with the language modeling loss. 2) General RM Fine-tuning (GRFT): Fine-tune the base models with the general preference data (H&H, WebGPT, GPT-4-LLM) using ranking loss and optionally imitation learning loss. 3) Customized RM Fine-tuning (CRFT): Fine-tune the general RMs with the DSP dataset using ranking loss and optionally imitation learning loss.
    Step 5: Get Results: Evaluate the RMs on the testing sets of H&H, WebGPT, GPT-4-LLM, and DSP datasets. Calculate the preference accuracy by counting the proportion of correct predictions over the testing set.
    Step 6: Analyze Results: Compare the performance of the RMs on the general and customized preference datasets. Analyze the impact of different training strategies, such as data enrichment and imitation learning, on preserving the general preferring ability while fitting the customized human preferences.



paper 1 meta review:
 This well written paper has been assessed by three knowledgeable reviewers who voted for its acceptance (one full accept and two marginal accepts). The reviewers liked the originality and potential impact of the showcased work in healthcare, yet had minor concerns about generality of the conclusion since the method was demonstrated on only one publicly available dataset. But in summary, this work meets the requirements for acceptance at ICLR 2024.
Justification for why not higher score: This is a good paper with some residual limitations, very well suited for a poster session.
Justification for why not lower score: This paper is above the ICLR acceptance threshold.

paper 1 score:  6.7



paper 2 meta review:
 The authors address the task of developing personalized reward models for language model alignment, highlighting the importance of customized reward models tailored to individual user preferences. The main contributions are the introduction of a novel synthetic preference dataset specifically designed for the customization of reward models, and a baseline training methodology employing multi-stage fine-tuning. The authors also explore the impact of different configurations of the baseline training methodology on the performance of these customized reward models.

Although the reviewers agree that the paper is interesting and makes a step in the right direction, they also raise a number of critical comments and concerns, including the limited novelty of the baseline approach, the synthetic nature and possible bias of the dataset, the lack of baselines and alternatives, the lack of policy-level evaluation, the insufficient discussion on personalisation, and questions regarding generalisation. In the discussion between authors and reviewers, some of these points could be resolved but other not. Eventually, there was a consensus that the paper does not yet reach the level expected for a top-venue such as ICLR.
Justification for why not higher score: Quality is not good enough.
Justification for why not lower score: N/A

paper 2 score:  4.3


accepted paper:  1




paper 1:
Meta-Knowledge Extraction:
  Title: Meta-Knowledge Extraction: Uncertainty-Aware Prompted Meta-Learning
  Problem Statement: Conventional meta-learning methods involve adapting all meta-knowledge to specific tasks, which incurs high computational costs due to the adaptation process. This paper addresses the need for a more efficient meta-learning framework that reduces computational costs while maintaining performance.
  Motivation: Existing meta-learning methods, particularly optimization-based ones, require significant computational resources to adapt meta-knowledge to each specific task. This is inefficient and impractical for large-scale applications. Inspired by the success of large language models in knowledge extraction, the authors propose a new paradigm that freezes the model backbone and uses task-specific prompts to extract meta-knowledge. This approach aims to reduce computational costs while preserving or even enhancing performance.
  Proposed Method: The proposed method, Uncertainty-Aware Prompted Meta-Learning (UAPML), introduces a meta-knowledge extraction paradigm. Instead of adapting the entire meta-knowledge, UAPML employs a learnable Bayesian meta-prompt to provide an ideal initialization for task-specific prompts. The model backbone is frozen, and task-specific prompts are used to extract meta-knowledge for few-shot tasks. The posterior uncertainty of the Bayesian meta-prompt is aligned with that of the task-specific prompt, which helps in modulating the construction of task-specific prompts. Two methods, soft and hard modulation, are proposed to construct task-specific prompts automatically.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: The experiments are conducted on a mixture of four sub-datasets: Aircraft, CIFAR-FS, Mini-Imagenet, and miniQuickDraw. Additionally, experiments are conducted on the vanilla datasets Mini-Imagenet and Tiered-Imagenet.
    Step 2: Construct Prompts: For the proposed method, a learnable Bayesian meta-prompt is used to initialize task-specific prompts. The task-specific prompts are then concatenated with the features from the frozen model backbone. Two modulation techniques, soft and hard, are used to construct the task-specific prompts based on the posterior uncertainty of the meta-prompt.
    Step 3: Select Models: The experiments use two model architectures: a 4-layer convolution network (Conv4) and a 12-layer ResNet (ResNet12). The meta-parameters include the model backbone, task classification head, and the merge module.
    Step 4: Get Results: Evaluate the performance of UAPML on the gathered datasets under 5-way 1-shot and 5-way 5-shot settings. Compare the results with several state-of-the-art baselines, including MAML, ANIL, BOIL, ProtoNet, Meta-SGD, Sparse-MAML, LEO, and Sharp-MAML.
    Step 5: Analyze Results: Analyze the performance in terms of accuracy and computational efficiency. Conduct ablation studies to evaluate the impact of the Bayesian meta-prompt and the uncertainty-aware construction methods. Visualize the task-specific prompts to understand how they capture task-specific information.


paper 2:
Future Language Modeling from Temporal Document History:
  Title: Future Language Modeling from Temporal Document History
  Problem Statement: The paper addresses the problem of predicting future textual data based on a temporal history of texts. This problem is interesting and important because while there are many automated systems for predicting future numerical data, there is relatively little work in automatically predicting textual data. Textual data predictions are valuable as they are a natural format for human consumption, and experts routinely make predictions in a textual format.
  Motivation: Existing methods for predicting future data are primarily focused on numerical data such as weather, stock prices, and product demand. There is a scarcity of work on automating the prediction of textual data, which is significant for human consumption and expert predictions. The proposed method aims to fill this gap by introducing the task of future language modeling, which involves probabilistic modeling of future texts based on a temporal history of texts. The proposed method is expected to work better than existing baselines by incorporating temporal information into pre-trained language models, thereby dynamically adjusting the generation probability to generate content that follows predicted future trends.
  Proposed Method: The proposed method involves developing future language models that incorporate temporal information into pre-trained language models. Three methods are developed: a word frequency model, a contextual temporal model, and a doubly contextualized temporal model. These models modify the language model probabilities to account for temporal evolution. The word frequency model uses an LSTM to predict temporal word biases based on historical word frequencies. The contextual temporal model uses contextualized embeddings averaged over all instances of a word over the year and feeds them into an LSTM to predict temporal word biases. The doubly contextualized model introduces a gating mechanism to decide when to rely on the temporal contextual model versus the prior state in the language model.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: Collect paper abstracts for each year from the ACL anthology website. Filter out noisy abstracts such as non-English papers. Use abstracts from 2003-2019 as training data, the year 2020 as development data, and the year 2021 as test data.
    Step 2: Model Selection: Use GPT-2 as the pre-trained language model. Develop and evaluate the following models: Baseline (fine-tunes GPT-2 on abstracts from all previous years), Baseline-n (fine-tunes GPT-2 on abstracts from the n most recent previous years), Frequency-NoLSTM (word frequency model without LSTM), Frequency (word frequency model with LSTM), Context (temporal contextual model), and Context2 (doubly contextualized model).
    Step 3: Training and Hyperparameter Tuning: Use the Adam optimizer with a batch size of 2 and gradient accumulation size of 2. Apply dropout with a probability of 0.1 between layers. Fine-tune for 10 epochs with early stopping. Set α to 1e-3 or initialize with 1 when automatically learned. Conduct hyperparameter search trials on α with values 1, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5. Use beam search decoding with top-k sampling (beam size 5, k 50, p 0.92).
    Step 4: Evaluation Metrics: Use three automatic evaluation metrics: Perplexity (PPL) to evaluate fluency, Content Perplexity (CPL) to evaluate the adequacy of future research ideas by filtering out stopwords, and Content Meteor (CM) to measure the match between model-generated abstracts and real abstracts in the dev and test sets.
    Step 5: Human Evaluation: Conduct a human evaluation by randomly evaluating 100 generated abstracts for each approach. Evaluate the abstracts with six criteria: Topic (clear and correct), Topic New (new topic), Problem (clear and correct), Problem New (new problem), Method (clear and correct), and Method New (new method). Each criterion score is binary (0 or 1).
    Step 6: Analyze Results: Compare the performance of the proposed methods against the baselines using both automatic and human evaluation metrics. Analyze whether the proposed methods generate content that is more related to future texts and whether they improve the fluency and adequacy of the generated abstracts.



paper 1 meta review:
 The paper proposes an Uncertainty-Aware Prompt Meta-learning (UAPML) framework, which improves the efficiency of gradient-based meta-learning by focusing on meta-knowledge extraction for few-shot tasks instead of computationally expensive meta-knowledge adaption. The soft and hard modulation techniques are developed to automatically generate task-specific prompts while considering the shared and task specific information among the few-shot tasks.  Experiments are conducted in various settings to demonstrate the effectiveness of the proposed UAPML framework. 

As pointed out by the reviewers, some key pieces of the proposed methodology are not clearly described, which lead to confusions around several important technical components. The evaluation results are not totally convincing as the improvements in the computation time are less significant compared to the baselines and experiments on large backbones are also missing. The paper should also more clearly highlight the distinctions from important related works (as pointed by the reviewers) to better justify the technical novelty.
Justification for why not higher score: The evaluation results are not entirely convincing and there are confusions about some key technical components, making it difficult to assess the overall contribution and novelty.
Justification for why not lower score: N/A

paper 1 score:  4.3



paper 2 meta review:
 This paper introduces the task of future language modeling and propose future language models. The proposed models are evaluated on the future abstract prediction task, and it outperforms the baseline non-temporal language models across all automatic evaluation metrics and human evaluation on generating content related to the future text based on temporal historical documents.
 
The task of future language modeling is new and interesting. The proposed language models sound reasonable and the results are promising. The paper is easy to follow. 
 
The weaknesses of this paper lie in the evaluation part. First, only one single artificial task (i.e. future abstract prediction) is used in the experiments, which is not sufficient. Additional experiments and evaluation on real downstream tasks need to be performed. Second, only GPT-2 (which is old) is used as the base model in the experiments.  It would be better to test the robustness to include a few more base models with larger sizes.
Justification for why not higher score: The evaluation is not very strong, and only one single artificial task is used for evaluation. The usability and effectiveness of the proposed model in real applications/tasks are not demonstrated.
Justification for why not lower score: The problem investigated in this pape is very ininteresting and the idea is new.

paper 2 score:  7.3


accepted paper:  2




paper 1:
Building Cooperative Embodied Agents Modularly with Large Language Models:
  Title: Building Cooperative Embodied Agents Modularly with Large Language Models
  Problem Statement: The paper addresses the challenge of multi-agent cooperation in decentralized settings with raw sensory observations, costly communication, and multi-objective tasks in various embodied environments. The goal is to develop agents that can efficiently plan, communicate, and cooperate to accomplish long-horizon tasks.
  Motivation: Existing methods either assume a cost-free communication channel or rely on a centralized controller with shared observations, which are not practical in real-world scenarios. The inspiration behind the new method is to leverage the commonsense knowledge, reasoning ability, language comprehension, and text generation capabilities of Large Language Models (LLMs) to build a modular framework that integrates perception, memory, and execution. The proposed method aims to surpass existing planning-based methods and exhibit emergent effective communication.
  Proposed Method: The proposed method, Cooperative Embodied Language Agent (CoELA), integrates LLMs into a cognitive-inspired modular framework. CoELA consists of five key modules: Perception, Memory, Communication, Planning, and Execution. The Perception Module processes raw sensory observations, the Memory Module stores knowledge and experience, the Communication Module generates messages using LLMs, the Planning Module makes high-level plans using LLMs, and the Execution Module generates primitive actions to execute the plans.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: The experiments are conducted on two embodied environments: ThreeDWorld Multi-Agent Transport (TDW-MAT) and Communicative Watch-And-Help (C-WAH). TDW-MAT involves transporting objects in a multi-room environment, while C-WAH involves household activities with symbolic and visual observations.
    Step 2: Construct Prompts: For the Communication Module, prompts are constructed with components like Instruction Head, Goal Description, State Description, Action History, and Dialogue History. For the Planning Module, prompts include the current state information and an Action List of available high-level plans. Example prompts for the Planning Module on C-WAH and TDW-MAT are provided in the appendix.
    Step 3: Select Models: The main experiments use GPT-4 from the OpenAI API. Additional experiments are conducted with LLAMA-2-13b-chat, and a fine-tuned version called CoLLAMA using LoRA with data collected from the agents.
    Step 4: Get Results: Evaluate the performance of CoELA on the test sets of TDW-MAT and C-WAH. Metrics include Transport Rate (TR) for TDW-MAT and Average Steps (L) for C-WAH. Efficiency Improvement (EI) is calculated as the difference in the main efficiency metric divided by the larger value of the metric for numerical stability.
    Step 5: Analyze Results: Compare the performance of CoELA with baseline methods like Rule-based Hierarchical Planner (RHP), MCTS-based Hierarchical Planner (MHP), and Multi-Agent Transformer (MAT). Analyze the effectiveness of communication, cooperation behaviors, and human-agent interaction through a user study. Identify failure cases and limitations of the current LLMs.


paper 2:
LLM Censorship: The Problem and its Limitations:
  Title: LLM Censorship: The Problem and its Limitations
  Problem Statement: Large language models (LLMs) have shown impressive capabilities in understanding and generating text based on complex instructions. However, their strict adherence to instructions raises concerns about their potential misuse for malicious purposes. The paper addresses the problem of effectively censoring LLM outputs to prevent the generation of harmful or impermissible content.
  Motivation: Existing methods such as model fine-tuning and output censorship using LLMs have proven to be fallible, as LLMs can still generate problematic responses. Common censorship approaches treat the issue as a machine learning problem, relying on another LLM to detect undesirable content. However, these methods have inherent limitations. The paper argues that semantic censorship is an undecidable problem due to the programmatic and instruction-following capabilities of LLMs. Additionally, knowledgeable attackers can reconstruct impermissible outputs from permissible ones, making censorship a security problem that requires security-based defenses.
  Proposed Method: The paper proposes reevaluating the problem of censorship and viewing it as a security problem. It suggests adapting security-based defenses to mitigate potential risks. The paper also introduces the concept of 'Mosaic Prompts,' an attack method where attackers construct impermissible outputs from a collection of permissible ones. The paper emphasizes the need for a new approach to censorship that goes beyond semantic detection and incorporates security principles.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: Collect datasets that include examples of both permissible and impermissible content. These datasets should cover a range of topics, including social engineering, data exfiltration, and other malicious activities.
    Step 2: Construct Prompts: For baseline, use direct prompting where given a query, the LLM generates a response without any censorship. For the proposed method, implement a security-based censorship mechanism that evaluates the permissibility of the generated content. Additionally, design prompts to test the effectiveness of 'Mosaic Prompts' by attempting to reconstruct impermissible outputs from permissible ones.
    Step 3: Select Models: Use state-of-the-art LLMs such as GPT-3.5, GPT-4, and other advanced models. Ensure that the models have been fine-tuned for instruction-following capabilities.
    Step 4: Implement Security-Based Censorship: Develop and integrate security-based censorship mechanisms that can evaluate the permissibility of LLM outputs. This may involve using access controls, user monitoring, and other security measures to prevent the generation of harmful content.
    Step 5: Test Mosaic Prompts: Design and execute experiments to test the effectiveness of 'Mosaic Prompts.' This involves providing the LLM with multiple permissible prompts and attempting to reconstruct impermissible outputs from the generated responses.
    Step 6: Get Results: Collect and analyze the results of the experiments. Evaluate the effectiveness of the security-based censorship mechanisms and the success rate of 'Mosaic Prompts' in bypassing censorship.
    Step 7: Analyze Results: Compare the performance of the proposed security-based censorship mechanisms with existing methods. Assess the limitations and potential risks associated with each approach. Provide insights and recommendations for improving censorship in LLMs.



paper 1 meta review:
 The authors tackle the problem of multi-agent cooperation focusing on the creation of a modular approach to agent construction that has explicit modules for every aspect of planning including communication. Overall reviewers were happy with the work though admit that it is a rather complex piece to evaluate.  They appreciate the modular framework, the experiments design and analysis, and the use of open LLMs.  Importantly, the work does include ablations to make the contributions of all modules clear in performance.
Justification for why not higher score: The reviewers are positive yet overall still cautious.  There are likely several reasons for this -- from the impact of each model/module to the complexity of experimental conditions, choices of environment.  It is difficult to fully disentangle the impacts of each decision/component.
Justification for why not lower score: Everyone is in agreement that the motivation is clear and that this is an interesting/important area for the community to tackle.  Nobody votes for rejection.

paper 1 score:  6.5



paper 2 meta review:
 The paper studies the problem of LLM censorship, which is to use an algorithm, potentially implemented by another LLM, to determine whether a string is permissible. The paper provides an interesting theoretical argument: LLM censorship is actually an “undecidable” problem, which means that no algorithm exists to fulfill this goal. 
The paper then presents a “Mosaic Prompts” method to bypass the LLM censorship, which decomposes an impermissible program into permissible components.
Finally, the paper argues that LLM censorship should be dealt with by security approaches instead of ML approaches.
Justification for why not higher score: The main reason that the reviewers reject this paper is due to limited contributions. The theorems are mostly from direct implications of existing ones. The paper also lacks sufficient evaluations to support the proposed Mosaic Prompts approach, which by far seems more like a proof of concept. While the paper's ideas are generally very interesting, as pointed out by one reviewer, it feels more like a "position" paper instead of a scientific contribution suitable for ICLR.
Justification for why not lower score: N/A

paper 2 score:  4.5


accepted paper:  1




paper 1:
Sampling Based Watermarking:
  Title: I Know You Did Not Write That! A Sampling Based Watermarking Method for Identifying Machine Generated Text
  Problem Statement: The proliferation of Large Language Models (LLMs) has led to potential harms such as mass misinformation and plagiarism. There is a need for a reliable method to detect machine-generated text to mitigate these risks.
  Motivation: Existing methods for detecting machine-generated text, such as classifiers and linguistic feature-based detectors, have limitations. They often require extensive training data, are biased against non-native English writers, and are generally ineffective. The proposed watermarking method aims to embed a unique pattern within the generated text, making it detectable algorithmically while maintaining textual quality. This method is expected to be more robust and detectable compared to existing state-of-the-art watermarking methods.
  Proposed Method: The proposed method involves intervening in the token sampling process during text generation. For each token to be generated, multiple candidate tokens are sampled based on their probability. A secret number is calculated for each candidate token, and the token with the highest secret number is selected. This process embeds a unique pattern in the text, which can be detected by calculating the secret numbers of tokens in the generated text and comparing them to a threshold.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: Use the train split of the 'realnewslike' portion of the C4 dataset and the train split for Wikitext (103-v1-raw) dataset. These datasets provide a diverse set of prompts and human-authored texts for comparison.
    Step 2: Select Models: Use three different models: OPT (1.3B parameters), BTLM-3B (3B parameters), and Llama2 (7B parameters). These models provide a range of capabilities for generating text.
    Step 3: Generate Text: Generate text using the selected models with the following configurations: i) without watermarking, ii) with the proposed sampling with replacement (SWR) method, iii) with the proposed sampling without replacement (SWOR) method, and iv) with the Maryland Watermarking (MWM) method. Ensure each generated text is 200 tokens long.
    Step 4: Evaluate Text Quality: Measure the quality of the generated text using P-SP (similarity to human-authored text), diversity (n-gram repetition rates), and coherence (semantic coherence between the prompt and the generated text).
    Step 5: Detect Watermark: Calculate the average z-scores for the generated texts using the proposed watermark detection method. Set the z-score threshold to 4 and calculate the percentage of texts detected as watermarked.
    Step 6: Robustness Testing: Conduct token-level paraphrasing attacks by masking a percentage of tokens in the watermarked text and replacing them using a DistilRoBERTa-Base model. Measure the detection rate after these attacks.
    Step 7: Analyze Results: Compare the performance of the proposed methods (SWR and SWOR) against the Maryland Watermarking method in terms of detectability, text quality, and robustness. Analyze the impact of different sampling counts and output distribution entropy on the watermarking performance.


paper 2:
Zero-Shot Robustification of Zero-Shot Models:
  Title: Zero-Shot Robustification of Zero-Shot Models
  Problem Statement: Zero-shot inference allows the use of large pretrained models for downstream classification tasks without further training. However, these models inherit biases from their training data, which can negatively impact their performance. The traditional solution of fine-tuning undermines the key advantage of pretrained models, which is their ability to be used out-of-the-box.
  Motivation: Existing methods to improve robustness in zero-shot models often require labeled data for training or fine-tuning, which is not suitable for zero-shot settings. Additionally, methods to debias word embeddings often require domain expertise and manual specification, limiting their applicability. The proposed method, RoboShot, leverages language models to obtain insights from task descriptions to improve robustness without supervision, training, or manual identification. This approach is expected to work better because it uses the inherent knowledge in language models to refine zero-shot representations, particularly improving performance on underperforming data slices.
  Proposed Method: RoboShot improves the robustness of pretrained model embeddings in a fully zero-shot fashion. It uses language models to extract insights from task descriptions, which are then embedded and used to remove harmful and boost useful components in embeddings. The method involves the following steps: 1) Obtain insights from language models using task descriptions. 2) Embed these insights to identify harmful and beneficial components in the embeddings. 3) Modify the embeddings to neutralize harmful components and emphasize beneficial ones. The method is theoretically supported by a model that quantifies failures in zero-shot models and characterizes the conditions under which RoboShot can boost performance.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: Use nine image and NLP classification tasks, including Waterbirds, CelebA, PACS, VLCS, and CXR14 for image classification, and CivilComments-WILDS, HateXplain, Amazon-WILDS, and Gender Bias for text classification.
    Step 2: Construct Prompts:
      Image Datasets:
        vharmfulprompt: List the biased/spurious differences between [classes].
        vhelpfulprompt: List the true visual differences between [classes].
      NLP Datasets:
        CivilComments-WILDS: Use demographic mentions annotations to construct vharmful.
        HateXplain: Use demographic mentions annotations to construct vharmful.
        Amazon-WILDS: What are the biased differences between good and bad amazon reviews?
        Gender Bias: What are the biased differences between comments about female and comments about male?
    Step 3: Select Models: Use a variety of models including CLIP (ViT-B-32 and ViT-L-14), ALIGN, AltCLIP for image classification, and BERT, Ada, BART-MNLI, and ChatGPT for text classification.
    Step 4: Get Results: Evaluate the performance of RoboShot on the selected datasets by comparing the average accuracy and worst-group accuracy against zero-shot baselines and group prompt zero-shot baselines.
    Step 5: Analyze Results: Compare the improvements in average accuracy and worst-group accuracy. Analyze the impact of removing harmful components and boosting helpful components in the embeddings. Additionally, evaluate the compatibility of RoboShot with different pretrained and language models.



paper 1 meta review:
 All the reviewers have concerns about the paper and are not in favor of acceptance. Most prominently, the comparison with Kirchenbauer et al. (2023a). The rebuttal period did not clear out the concerns.
Justification for why not higher score: All the reviewers are negative about the paper.
Justification for why not lower score: N/A

paper 1 score:  3.7



paper 2 meta review:
 This paper presents RoboShot, an innovative method that improves the robustness of pretrained model embeddings in a fully zero-shot fashion by leveraging insights from large language models to refine embeddings and address inherited biases. The theoretical framework provided characterizes the conditions under which RoboShot can outperform existing methods in zero-shot learning. The experimental evaluation is well-structured and rigorous, demonstrating consistent improvements over several zero-shot baselines across multiple datasets and model architectures. Additionally, the paper is clearly written and accessible, making it a valuable contribution to the field. Based on these strengths, I recommend acceptance of this paper.
Justification for why not higher score: I think this is definitely a solild paper and a great contribution to the community. However, I think in terms of the methodology side, it doesn't sound super exciting (it can be similiar to a lot of methods using LMs recently). Therefore, I recommend the current score.
Justification for why not lower score: n/a

paper 2 score:  7.5


accepted paper:  2




paper 1:
Large Language Models as Rational Players in Competitive Economics Games:
  Title: Large Language Models as Rational Players in Competitive Economics Games
  Problem Statement: The paper addresses the problem of evaluating the rationality and strategic reasoning abilities of large language models (LLMs) in dynamic environments. Traditional static benchmarks fail to capture the adaptive responses of LLMs in competitive settings, making it essential to explore new evaluation methods.
  Motivation: Existing methods for evaluating LLMs rely on static benchmarks or human-judged A/B tests, which do not reflect the models' ability to adapt to dynamic environments. Inspired by the emergent capabilities of LLMs in economics games, the authors propose using competitive games as a dynamic benchmark. This approach aims to provide a more comprehensive evaluation of LLMs' rationality, strategic reasoning, and instruction-following capabilities.
  Proposed Method: The authors propose an economics arena (EconArena) where LLMs participate in competitive games such as beauty contests and private-value second-price auctions. By varying the game history revealed to LLM-based players, the study evaluates the rationality and strategic reasoning abilities of different LLMs. The method involves tracking the strategies and payoffs of LLMs to measure their performance against Nash Equilibria (NE) and their ability to adapt to dynamic environments.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: The experiments involve two types of competitive games: beauty contests and private-value second-price auctions. The beauty contest games require players to choose a number closest to two-thirds of the average of all chosen numbers. The auction games involve bidding for an item with private values and paying the second-highest bid.
    Step 2: Construct Prompts:
      Beauty Contest in the Melee Environment:
        Prompt: [SYSTEM] I want you to act as a clever game player and perform as rationally as possible. [USER] You are playing a game and the rules are as follows: • it consists of {number of players} players, including you. • this is a one-round game. • you need to choose a real number between 0 and {upper bound}, 0 and {upper bound} inclusive. • everyone does not know how others would make choices beforehand. • the winner is the person whose number is the closest to the two-thirds of the average of all chosen numbers. • the winner gets a fixed prize, in case of a tie the prize is split amongst those who tie. • your goal is primarily to maximize the possibility of getting the prize and secondly to maximize your prize. Please just strictly output a JSON string, which has following keys: • understanding: str, your brief understanding of the game • popular answer: float, the number which you think other players are most likely to choose • answer: float, the number which you would like to choose • reason: str, the brief reason why you give the popular answer and the answer that way
      Beauty Contest in the Rational Environment:
        Prompt: [SYSTEM] I want you to act as a clever game player and perform as rationally as possible. [USER] You are playing a game and the rules are as follows: • it consists of {number of players} players, including you. • this is a one-round game. • you need to choose a real number between 0 and {upper bound}, 0 and {upper bound} inclusive. • everyone does not know how others would make choices beforehand. • the winner is the person whose number is the closest to the two-thirds of the average of all chosen numbers. • the winner gets a fixed prize, in case of a tie the prize is split amongst those who tie. • your goal is primarily to maximize the possibility of getting the prize and secondly to maximize your prize. • you can assume that other are all perfectly rational players. Please just strictly output a JSON string, which has following keys: • understanding: str, your brief understanding of the game • popular answer: float, the number which you think other players are most likely to choose • answer: float, the number which you would like to choose • reason: str, the brief reason why you give the popular answer and the answer that way
      Beauty Contest with Chain of Thought Technique:
        Prompt: [SYSTEM] I want you to act as a clever game player and perform as rationally as possible. [USER] You are playing a game and the rules are as follows: • it consists of {number of players} players, including you. • this is a one-round game. • you need to choose a real number between 0 and {upper bound}, 0 and {upper bound} inclusive. • everyone does not know how others would make choices beforehand. • the winner is the person whose number is the closest to the two-thirds of the average of all chosen numbers. • the winner gets a fixed prize, in case of a tie the prize is split amongst those who tie. • your goal is primarily to maximize the possibility of getting the prize and secondly to maximize your prize. • you can assume that other are all perfectly rational players. Let’s think step by step. After that, please output a JSON string, which has following keys: • popular answer: float, the number which you think other players are most likely to choose • answer: float, the number which you would like to choose
      Beauty Contest with Historical Information:
        Prompt: [SYSTEM] I want you to act as a clever game player and perform as rationally as possible. [USER (run 1)] You are playing a game and the rules are as follows: • it consists of {number of players} players, including you. • this is a one-round game. • you need to choose a real number between 0 and {upper bound}, 0 and {upper bound} inclusive. • everyone does not know how others would make choices beforehand. • the winner is the person whose number is the closest to the two-thirds of the average of all chosen numbers. • the winner gets a fixed prize, in case of a tie the prize is split amongst those who tie. • your goal is primarily to maximize the possibility of getting the prize and secondly to maximize your prize. Please just strictly output a JSON string, which has following keys: • understanding: str, your brief understanding of the game • popular answer: float, the number which you think other players are most likely to choose • answer: float, the number which you would like to choose • reason: str, the brief reason why you give the popular answer and the answer that way [USER (runs after run 1)] The game of the same config has been hold for {number of runs} run(s), and the historical choices of everyone are shown below (your id is {ID of the player}): {historical information (in JSON format)} Everyone can optimize his/her answer with the history to play in a new run in order to achieve goals. Please just strictly output a JSON string for a new run, which has following keys: • goal: str, briefly check if you still remember what goal you should achieve in the game • previous answer: float, the number which you chose in the last run • answer: float, the number which you would like to adjust your choice to • reason: str, the brief reason why you adjust the answer that way
      Second-Price Auction in the Melee Environment:
        Prompt: [SYSTEM] I want you to act as a smart auction bidder and perform as rationally as possible. [USER] You are participating in an auction and the rules are as follows: • it consists of {number of bidders} bidders, included you. • this is a one-round auction. • there is only 1 item, and your private value of the item is {private value of the bidder} units (private values may vary among bidders). • you have {assets of the bidder} units of assets, and you need to place a bid which is not higher than your assets. • everyone does not know either private value of others or how others would make choices beforehand. • the bidder who places the highest bid among all the bids will get the item (others will not), and only need to pay an amount of assets equalling to the second-highest bid among all the bids. • if there are multiple highest bid, only the bidder with the minimal id will get the item. • if you get the item, your payoff equals to your remaining assets (assets deducting payment) plus your private value, otherwise your payoff equals to your original assets. • your goal is to maximize your overall payoffs (notice that getting the item is not necessary) considering the situation unpredictable. Please just strictly output a JSON string, which has following keys: • understanding: str, your brief understanding of the auction • bid: float, the bid which you would like to place • reason: str, the brief reason why you place the bid that way
      Second-Price Auction in the Rational Environment:
        Prompt: [SYSTEM] I want you to act as a smart auction bidder and perform as rationally as possible. [USER] You are participating in an auction and the rules are as follows: • it consists of {number of bidders} bidders, included you. • this is a one-round auction. • there is only 1 item, and your private value of the item is {private value of the bidder} units (private values may vary among bidders). • you have {assets of the bidder} units of assets, and you need to place a bid which is not higher than your assets. • everyone does not know either private value of others or how others would make choices beforehand. • the bidder who places the highest bid among all the bids will get the item (others will not), and only need to pay an amount of assets equalling to the second-highest bid among all the bids. • if there are multiple highest bid, only the bidder with the minimal id will get the item. • if you get the item, your payoff equals to your remaining assets (assets deducting payment) plus your private value, otherwise your payoff equals to your original assets. • your goal is to maximize your overall payoffs (notice that getting the item is not necessary) considering the situation unpredictable. • you can assume that others are all perfectly rational bidders. Please just strictly output a JSON string, which has following keys: • understanding: str, your brief understanding of the auction • bid: float, the bid which you would like to place • reason: str, the brief reason why you place the bid that way
      Second-Price Auction with Chain of Thought Technique:
        Prompt: [SYSTEM] I want you to act as a smart auction bidder and perform as rationally as possible. [USER] You are participating in an auction and the rules are as follows: • it consists of {number of bidders} bidders, included you. • this is a one-round auction. • there is only 1 item, and your private value of the item is {private value of the bidder} units (private values may vary among bidders). • you have {assets of the bidder} units of assets, and you need to place a bid which is not higher than your assets. • everyone does not know either private value of others or how others would make choices beforehand. • the bidder who places the highest bid among all the bids will get the item (others will not), and only need to pay an amount of assets equalling to the second-highest bid among all the bids. • if there are multiple highest bid, only the bidder with the minimal id will get the item. • if you get the item, your payoff equals to your remaining assets (assets deducting payment) plus your private value, otherwise your payoff equals to your original assets. • your goal is to maximize your overall payoffs (notice that getting the item is not necessary) considering the situation unpredictable. • you can assume that others are all perfectly rational bidders. Let’s think step by step. After that, please output a JSON string, which has following keys: • bid: float, the bid which you would like to place
      Second-Price Auction with Historical Information:
        Prompt: [SYSTEM] I want you to act as a smart auction bidder and perform as rationally as possible. [USER (run 1)] You are participating in an auction and the rules are as follows: • it consists of {number of bidders} bidders, included you. • this is a one-round auction. • there is only 1 item, and your private value of the item is {private value of the bidder} units (private values may vary among bidders). • you have {assets of the bidder} units of assets, and you need to place a bid which is not higher than your assets. • everyone does not know either private value of others or how others would make choices beforehand. • the bidder who places the highest bid among all the bids will get the item (others will not), and only need to pay an amount of assets equalling to the second-highest bid among all the bids. • if there are multiple highest bid, only the bidder with the minimal id will get the item. • if you get the item, your payoff equals to your remaining assets (assets deducting payment) plus your private value, otherwise your payoff equals to your original assets. • your goal is to maximize your overall payoffs (notice that getting the item is not necessary) considering the situation unpredictable. Please output a JSON string, which has following keys: • understanding: str, your brief understanding of the auction • bid: float, the bid which you would like to place • reason: str, the brief reason why you place the bid that way [USER (runs after run 1)] The auction of the same configuration has been hold for {number of runs} run(s), and the historical information of everyone are shown below (your id is {ID of the bidder}): {historical information (in JSON format)} Everyone can optimize his/her bid with the history to place bid in a new run in order to achieve the goal. Notice that everyone’s original asset and private value will stay unchanged. Please just strictly output a JSON string for a new run, which has following keys: • goal: str, briefly check if you still remember what goal you should achieve in the game • previous bid: float, the bid which you placed in the last run • previous payoff: float, the payoff which you got in the last run • bid: float, the bid which you would like to adjust to • reason: str, the brief reason why you adjust the bid that way
    Step 3: Select Models: The experiments involve the following LLMs: GPT-4, GPT-3.5, Claude2, Claude-I (Claude-Instant-1.2), PaLM2, Llama2, Baichuan2, ChatGLM2, and ChatGLM3.
    Step 4: Get Results: Collect the payoffs, strategies, and rule-following behaviors of the LLMs in the different game setups. Track the deviation from Nash Equilibria and the ability to adapt to dynamic environments.
    Step 5: Analyze Results: Compare the performance of different LLMs in terms of rationality, strategic reasoning ability, and instruction-following capability. Analyze the impact of game history on the convergence to optimal strategies and the ability to reason about other players' strategies.


paper 2:
Benchmarking and Improving Generator-Validator Consistency of Language Models:
  Title: Benchmarking and Improving Generator-Validator Consistency of Language Models
  Problem Statement: Language models (LMs) often produce inconsistent responses when generating and validating answers. For example, ChatGPT correctly answers 'what is 7+8' with '15', but responds 'False' when asked '7+8=15, True or False'. This inconsistency, termed generator-validator consistency (GV-consistency), undermines trust in LMs.
  Motivation: Existing LMs, including state-of-the-art models like GPT-4, exhibit significant GV-inconsistency, with GPT-4 being consistent only 76% of the time. This inconsistency is problematic as it erodes user trust and limits the reliability of LMs in various applications. The proposed method, consistency fine-tuning, aims to improve GV-consistency by finetuning LMs on filtered data that are GV-consistent, thereby enhancing both generator quality and validator accuracy without requiring labeled data.
  Proposed Method: The proposed method involves a two-stage process: data generation and consistency fine-tuning. In the data generation stage, the LM is prompted with generator queries to produce responses and then with validator queries to check the correctness of these responses. Only the consistent generator-validator pairs are retained. In the consistency fine-tuning stage, the LM is finetuned on these consistent pairs to improve GV-consistency. This process can be iterated to further enhance consistency.
  Step-by-Step Experiment Plan:
    Step 1: Gather Datasets: The experiments are conducted on six tasks: arithmetic, plan arithmetic, question answering (QA), harmful questions, prompt prioritization, and style transfer. These tasks cover a range of skills including arithmetic reasoning, knowledge, text editing, and instruction following.
    Step 2: Construct Prompts: For each task, specific generator and validator prompts are designed. For example, in arithmetic, the generator is prompted to produce a correct or incorrect answer, and the validator checks the correctness. In style transfer, the generator rewrites a sentence in a specified style, and the validator judges which of two sentences better matches the style.
    Step 3: Select Models: The primary model used for consistency fine-tuning is Alpaca-30B. Additional experiments include Alpaca-7B and comparisons with other models like GPT-3.5-turbo, GPT-4, and text-davinci-003.
    Step 4: Data Generation: Generate responses from the LM for both generator and validator prompts. Collect the generator-validator pairs and filter out inconsistent pairs, retaining only those that are GV-consistent.
    Step 5: Consistency Fine-Tuning: Finetune the LM on the filtered consistent pairs using the MLE objective. This involves optimizing the likelihood of the generator and validator responses conditioned on their respective prompts. Iterate this process to further improve consistency.
    Step 6: Evaluate GV-Consistency: Evaluate the GV-consistency of the finetuned model across the six tasks. Measure the percentage of consistent generator-validator pairs.
    Step 7: Analyze Results: Compare the GV-consistency, generator quality, and validator accuracy of the finetuned model against the baseline models. Assess the improvements in consistency and performance across both seen and unseen tasks and domains.



paper 1 meta review:
 The paper studies an interesting topic, i.e., the strategic reasoning ability of LLMs by letting the play in competitive games. This is a very interesting question. The AC appreciated and acknowledges the effort the authors put during the rebuttal to improve their manuscript based on the reviewers comments. However it seems that the reviewers (unanimously) were not convinced that the paper is above the bar for ICLR due to the limitations of the experiments.
Justification for why not higher score: The reviewers unanimously believe that the paper is below the bar and that further revisions are needed (with more experiments).
Justification for why not lower score: Not applicable

paper 1 score:  3.0



paper 2 meta review:
 This paper finds the inconsistency between generating and validating an answer is prevalent in language models (LMs), i.e., a violation of GV-consistency, and proposes a framework for measuring the GV-consistency of LLMs. To improve consistency, it proposes consistency fine-tuning. The effectiveness of consistency fine-tuning is validated across 6 tasks. 

The problem investigated in this paper is interesting and the proposed framework is useful for future researches. The proposed consistency fine-tuning method is reasonable and effective. 

Weaknesses: The work is very empirical and lacks theoretical depth. The dataset and experiments need more analyses and explanations.
Justification for why not higher score: see the meta-review.
Justification for why not lower score: see the meta-review.

paper 2 score:  6.7


accepted paper:  2