Example 1: 

For an analysis project on studying calibration of confidence levels in language models when faced with code-switched prompts, this will involve comparing the confidence levels of language models in code-switched versus monolingual prompts and examining the impact of different language pairs on model confidence. 

The test cases should include a monolingual sentence and a code-switched version as comparison. For example:

Monolingual sentence: "The weather is very hot today, what should I wear?"
Code-Switched version: "El clima está muy caliente hoy, what should I wear?"

Monolingual sentence: "Where is the capital of China?"
Code-Switched version: "中国的capital在哪儿?"

Then, use the language models to generate responses to the prompts above. Record the models' output probabilities or log-likelihood scores as a proxy for confidence and compare. 


Example 2:

For a method paper that proposes the idea of using energy-efficient prompts to generate computationally efficient code by large language models, it involves comparing computational efficiency of code generated by the proposed prompt versus the baseline prompt. 

The test cases should contain a coding problem, a baseline prompt, and the proposed energy-efficient prompt. For example, the coding problem could be to sort a list of integers as input. 

Baseline prompt: "Write a Python function to take a list of integers as input and return the list sorted in descending order."

Energy efficient prompt: "Write a Python function to take a list of integers as input and return the list sorted in descending order, use the most efficient algorithm possible with lowest time complexity."

Then, compare the efficiency of the two generated programs by running the programs on a set of test cases. 
