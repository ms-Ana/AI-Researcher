{
    "Self-Refinement Prompting": {
        "Title": "SELF-REFINE: Iterative Refinement with Self-Feedback",
        "Problem Statement": "Like humans, large language models (LLMs) do not always generate the best output on their first try. We are interested in how to leverage the LLM itself to improve its own output.",
        "Motivation": "LLMs may produce an intelligible initial output, yet may benefit from further iterative refinement — i.e., iteratively mapping a candidate output to an improved one — to ensure that the desired quality is achieved. Iterative refinement typically involves training a refinement model that relies on domain-specific data. Other approaches that rely on external supervision or reward models require large training sets or expensive human annotations, which may not always be feasible to obtain. Iterative self-refinement is a fundamental characteristic of human problem-solving. Iterative self-refinement is a process that involves creating an initial draft and subsequently refining it based on self-provided feedback. For example, When writing code, a programmer may implement an initial \"quick and dirty\" implementation, and then, upon reflection, refactor their code to a solution that is more efficient and readable.",
        "Proposed Method": "Inspired by this principle, we propose Self-Refine: an iterative self-refinement algorithm that alternates between two generative steps – FEEDBACK and REFINE. Given an initial output generated by a model M, we pass it back to the same model M to get feedback. Then, the feedback is passed back to the same model to refine the previously-generated draft. This process is repeated either for a specified number of iterations or until M determines that no further refinement is necessary. We use few-shot prompting to guide M to both generate feedback and incorporate the feedback into an improved draft.",
        "Step-by-Step Experiment Plan": {
            "Step 1: Gather Datasets": "We evaluate on the GSM8K dataset for mathematical problem solving, and the HumanEval dataset for code generation.",
            "Step 2: Construct Prompts": "For baseline, we prompt the model to generate the output directly. For the proposed method, we first prompt the model to generate an initial answer, then we prompt the same model with few-shot examples to generate feedback on the initial draft, and finally we prompt the model to refine the initial draft conditioned on the input question, original response, and the feedback. We repeat this loop until a stopping condition is met (e.g., set a max number of steps).",
            "Step 3: Select Models": "We use three main strong base LLM across all tasks: GPT-3.5 (text-davinci-003), ChatGPT (gpt-3.5-turbo), and GPT-4.",
            "Step 4: Get Results": "Get answer predictions from the models on these datasets with both the baselines and proposed method.",
            "Step 5: Analyze Results": "Compare whether the new method improves the performance of LLMs in these tasks."
        }
    },
    "Selective Attention Prompting": {
        "Title": "Reading Selectively: Selective Attention Prompting for Large Language Models",
        "Problem Statement": "Large Language Models (LLMs) are highly capable, yet they are still susceptible to making simple mistakes, which seem to display weak reasoning abilities. For example, they can be swayed to make erroneous judgments by irrelevant context, or by preference or opinion inherent in the input prompt, in the latter case exhibiting an issue termed sycophancy whereby the model agrees with the input.",
        "Motivation": "While several approaches try to mitigate these issues through adding more supervised training data or reinforcement learning strategies, we posit that the underlying problem is inherent in the way the transformer itself is built, and in particular its attention mechanism. That is, soft attention tends to assign probability to a large portion of the context, including irrelevant portions, tends to overly focus on repeated tokens partly due to the way it is trained, and partly due to the position encoding mechanism is also inclined to treat the context as a bag-of-words when it should not. We want to resolve this by using the LLM as a natural language reasoner to select which parts of the contexts to read.",
        "Proposed Method": "In this work, we investigate a radically different approach to attention mechanisms: performing attention by using the LLM as a natural language reasoner. Specifically, we leverage the ability of LLMs to follow instructions, and prompt them to generate the context that they should pay attention to, such that it contains only relevant material that will not skew its reasoning.",
        "Step-by-Step Experiment Plan": {
            "Step 1: Gather Datasets": "We evaluate on: 1) the modified TriviQA dataset that includes distractor opinion in the question; 2) on longform generation of arguments (SycophancyEval) that contain distractor input sentiment; and 3) on math word problems from GSM8K with in-topic irrelevant sentences. These tasks can help us understand the impact of context selection.",
            "Step 2: Construct Prompts": "We have two baselines: (1) no context: only give the question; (2) full context: give the full context. For the proposed method, we prompt the model to generate the context that it should pay attention to such that irrelevant parts of the context that will adversely affect the output are removed, and then we prompt the model to generate the answer conditioned on the input question and the generated context.",
            "Step 3: Select Models": "We test GPT-3.5 (Text-Davinci-003) and GPT-4 from the OpenAI API, as well as the open-source LLaMA-2-70B-chat.",
            "Step 4: Get Results": "Get answer predictions from the models on these datasets with both the baselines and proposed method.",
            "Step 5: Analyze Results": "Compare whether the new method improves the performance of LLMs in these tasks as compared to the baselines."
        }
    },
    "Chain-of-Verification Prompting": {
        "Title": "Chain-of-Verification Reduces Hallucination in Large Language Models",
        "Problem Statement": "Generation of plausible yet incorrect factual information, termed hallucination, is an unsolved issue in large language models.",
        "Motivation": "A majority of the methods for reducing hallucination can be divided into roughly three categories: training-time correction, generation-time correction and via augmentation (tool-use). We want to take a simpler approach that fully leverages the power of LLM itself. Our key motivation is that large language models, when suitably prompted, can both generate and execute a plan of how to verify themselves in order to check their own work, and finally incorporate this analysis into an improved response.",
        "Proposed Method": "Our overall process, which we call Chain-of-Verification (CoVe), thus performs four core steps: 1. Generate Baseline Response: Given a query, generate the response using the LLM. 2. Plan Verifications: Given both query and baseline response, generate a list of verification questions that could help to self-analyze if there are any mistakes in the original response. 3. Execute Verifications: Answer each verification question in turn, and hence check the answer against the original response to check for inconsistencies or mistakes. 4. Generate Final Verified Response: Given the discovered inconsistencies (if any), generate a revised response incorporating the verification results. Each of these steps is performed by prompting the same LLM in different ways to obtain the desired response.",
        "Step-by-Step Experiment Plan": {
            "Step 1: Gather Datasets": "We choose datasets that evaluate factual correctness, including MultiSpanQA dataset on closed-book QA and FactScore dataset on generating biographies.",
            "Step 2: Construct Prompts": "For baseline, we use direct prompting where given a query, we generate left-to-right as usual using the LLM, with no special tricks. Given such baseline generations are typically prone to hallucination, CoVe attempts to identify these hallucinations, and correct them, in the following steps.\n(1) Plan Verifications: Conditioned on the original query and the baseline response, the model is prompted to generate a series of verification questions that test the factual claims in the original baseline response.\n(2) Execute Verifications: Given the planned verification questions, the next step is to answer them in order to assess if any hallucinations exist. The planning prompt conditions on the baseline response in the first step. The verification questions generated from planning are answered in the second step, where crucially the context given to the LLM prompt only contains the questions, and not the original baseline response and hence cannot repeat those answers directly.\n(3) Generate Final Verified Response: Finally, the improved response that takes verification into account is generated. This is executed by a final few-shot prompt where the context takes into account all of the previous reasoning steps, the baseline response and verification question answer pairs, so that the corrections can take place.",
            "Step 3: Select Models": "We test GPT-3.5 (Text-Davinci-003) and GPT-4 from the OpenAI API, as well as the open-source LLaMA-2-70B-chat.",
            "Step 4: Get Results": "Get answer predictions from the models on these datasets with both the baselines and proposed method.",
            "Step 5: Analyze Results": "Compare whether the new method improves the performance of LLMs in these tasks as compared to the baselines."
        }
    }
}