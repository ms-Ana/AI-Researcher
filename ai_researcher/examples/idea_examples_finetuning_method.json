{
	"Improving Generator-Validator Consistency": {
		"Type": "finetuning",
		"Problem": "Language models (LMs) can generate high-quality responses to task prompts; however, the same model can sometimes produce contradictory responses when validating its own answers.",
		"Existing Methods": "Prior work has explored prompt consistency, and finetuned the LMs to improve the prediction similarity across different prompt rephrasings. Also, some works enforce logical consistency by selecting answers that are logically consistent with most of the other LM-generated statements. This work explores the new aspect of generator-validator consistency, which is applicable to a broad set of scenarios because most generative tasks have a corresponding verification task.",
		"Motivation": "As of September 2023, ChatGPT correctly answers \"what is 7+8\" with 15, but when asked \"7+8=15, True or False\" it responds with \"False\". This inconsistency between generating and validating an answer is prevalent in language models (LMs) and erodes trust. Therefore, we want to propose a framework for measuring the consistency between generation and validation (which we call generator-validator consistency, or GV-consistency), and to improve such consistency of LMs via finetuning.",
		"Proposed Method": "(1) Evaluating: In order to systematically assess GV-consistency of LMs, we begin by prompting the LM with a generator query to solicit an answer to a question, and then prompting the same LM with a validator query to check whether the generated answer is correct. Simply asking the validator for a correctness judgment can fail, as the trivial baseline of always answering \"correct\" has perfect performance. Our work avoids this degeneracy by randomizing the labels corresponding to the consistent answer. (2) Finetuning: To improve GV-consistency, we propose a simple procedure called consistency fine-tuning, which consists of a data generation stage and a fine-tuning stage. Given a generator and a validator prompt, we first query the generator to obtain the generator response, then query the validator to check the correctness of the generated response. We then filter the paired generator and discriminator responses to keep only the pairs that are GV-consistent. Finally, we finetune the LM to maximize the likelihood of the consistent pairs. This algorithm can be applied for multiple rounds.",
		"Experiment Plan": "To evaluate consistency fine-tuning, we experiment on 6 tasks, ranging from classic NLP tasks (style transfer and QA) to arithmetic reasoning (arithmetic and plan arithmetic) and instruction-following (harmful question and prompt prioritization). We compare the GV-consistency of the original LMs with the GV-consistency of the LMs after consistency fine-tuning. We can also evaluate whether the consistency finetuning can improve the generator generation quality and the validator accuracy."
	},
	"Finetuning for Factuality": {
		"Type": "finetuning",
		"Problem": "Language models are prone to making convincing but factually inaccurate claims, often referred to as ‘hallucinations.’ We want to ask: \"Can language models be fine-tuned to leverage this internal awareness, to avoid making untrue statements?\"",
		"Existing Methods": "In principle, reinforcement learning-based objectives can avoid the failures of existing pre-training objectives through the appropriate choice of a reward function that penalizes factually incorrect statements. However, accurately computing such a reward function can be expensive. Obtaining human labels of factuality is time-consuming and costly.",
		"Motivation": "To obtain factuality rewards, we can leverage recent advances in estimating truthfulness without human intervention: a) reference-based automated fact-checking methods that evaluate the extent to which an external knowledge base supports the claims in a piece of text; and b) reference-free truthfulness evaluations that use a model’s own confidence as a proxy for truthfulness. We can then use these rewards to fine-tune the language model to avoid making untrue statements.",
		"Proposed Method": "We will generate factuality preferences so that we can use existing preference learning algorithms like DPO to finetune the language model. We leverage two classes of approaches to generate such preferences without human labeling effort. First, we use FactScore as a representative method of reference-based truthfulness scoring. To evaluate a piece of text, FactScore first extracts a list of the atomic claims present in the text using GPT-3.5. For each atomic claim, a smaller, more efficient model such as a Llama-1-7B model that has been fine-tuned for fact-checking is then used to perform natural language inference to determine if a claim is supported by the reference text. The passage’s truthfulness score is the fraction of the extracted atomic claims that are estimated to be supported by the reference text. Second, to eliminate the need for external knowledge, we leverage the fact that large language models are well-calibrated to get reference-free confidence-based truthfulness estimation. We first perform a claim extraction step, as in reference-based methods, and compute the average confidence of the model over all extracted factual claims as the final truthfulness score. Given a choice of truthfulness estimator, we can now construct a preference dataset for factuality tuning a given language model from a set of unlabeled prompts. First, we sample n multiple candidate responses for each prompt from the model with simple temperature sampling with temperature 1.0 (using few-shot prompting for models that have not been fine-tuned). For each response, we then compute the truthfulness score with the chosen estimator (reference-based or reference-free). Finally, for all pairs of responses to each prompt, we simply choose the response with the higher truthfulness score as the preferred response. Finally, we fine-tune the model using the DPO pipeline, using all model responses as targets for the SFT stage.",
		"Experiment Plan": "We conduct our experiments on two tasks: generating biographies and medical question-answering. For biographies, we generated a dataset consisting of 355 diverse well-known individuals (296 train, 59 test) with 10 short-paragraph biographies each. For medical question answering, we used a dataset of 200 diverse common medical conditions (150 train, 50 test) with 6 questions about each condition and 6 short-paragraph answers per question. To evaluate each generated response, we follow the FactScore procedure to extract the number of correct and incorrect facts. For each dataset, we report the number of correct and relevant facts (# Correct), the number of inaccuracies (# Incorrect), and the proportion of correct relevant facts out of the total number of extracted facts (% Correct). We compare with the baseline of not doing the factuality tuning as well as the baseline of only doing supervised finetuning (SFT)."
	},
	"Bootstrapping Reasoning With Reasoning": {
		"Type": "finetuning",
		"Problem": "Generating step-by-step \"chain-of-thought\" rationales improves language model performance on complex reasoning tasks like mathematics or commonsense question-answering. However, inducing language model rationale generation currently requires either constructing massive rationale datasets or sacrificing accuracy by using only few-shot inference.",
		"Existing Methods": "One approach to rationale generation is the construction of a fine-tuning dataset of rationales, either manually by human annotators or automatically with hand-crafted templates. Manual methods are expensive, and it is infeasible to construct such a dataset for each interesting problem. Meanwhile, template-based methods rely on automatically-generated rationales but only work when a general solution is already known or reasonable hard-coded heuristics can be made. An alternative is to leverage in-context learning by including only a few rationale examples in the language model prompt. This has been shown to improve accuracy on mathematical and symbolic reasoning tasks relative to prompting without rationales (“direct” prompting). Yet, while fewshot techniques with rationales tend to outperform their non-reasoning counterparts, they generally substantially underperform models fine-tuned to directly predict answers using larger datasets.",
		"Motivation": "To address these, we adopt a different approach: by leveraging the LLM’s pre-existing reasoning ability, we iteratively bootstrap the ability to generate high-quality rationales.",
		"Proposed Method": "Specifically, we few-shot prompt a large language model to self-generate rationales and refine the model’s ability further by fine-tuning on those rationales that lead to correct answers. We repeat this procedure, using the improved model to generate the next training set each time. This is a synergistic process, where improvements in rationale generation improve the training data, and improvements in training data further improve rationale generation. We further propose rationalization: for each problem that the model fails to answer correctly, we generate a new rationale by providing the model with the correct answer. This lets the model reason backward—given the correct answer, the model can more easily generate a useful rationale. These rationales are then collected as part of the training data, which further improves overall accuracy. In our method, we repeat the following process: in each iteration, first construct a finetuning dataset by attempting to solve the dataset using the current model’s rationale generation ability; then, augment this dataset using rationalization, justifying ground-truth answers to problems the model failed to solve; finally, finetune the large language model on the combined dataset.",
		"Experiment Plan": "We apply STaR on arithmetic, math word problems (GSM-8K), and commonsense reasoning (CommonsenseQA). We use GPT-J as the base model, and compare the accuracy of few-shot direct prompting, few-shot CoT prompting, direct finetuning, and finetuning with STaR."
	},
	"Self Instruction Tuning": {
		"Type": "finetuning",
		"Problem": "Large “instruction-tuned” language models (i.e., finetuned to respond to instructions) have demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless, they depend heavily on human-written instruction data that is often limited in quantity, diversity, and creativity, therefore hindering the generality of the tuned model.",
		"Existing Methods": "Recent instruction tuning research developments are powered by two key components: large pretrained language models (LM) and human-written instruction data (e.g., PROMPTSOURCE and SUPERNATURALINSTRUCTIONS). However, collecting such instruction data is costly and often suffers limited diversity given that most human generations tend to be popular NLP tasks, falling short of covering a true variety of tasks and different ways to describe them.",
		"Motivation": "Continuing to improve the quality and coverage of instruction-tuned models necessitates the development of alternative approaches for supervising the instruction tuning process, ideally without relying on human-written instruction data. For that, we can leverage LLMs themselves to generate and filter high-quality instruction tuning data.",
		"Proposed Method": "In this work, we introduce SELF-INSTRUCT, a semi-automated process for instruction-tuning a pretrained LM using instructional signals from the model itself. The overall process is an iterative bootstrapping algorithm, which starts off with a limited seed set of manually-written tasks that are used to guide the overall generation. In the first phase, the model is prompted to generate instructions for new tasks. This step leverages the existing collection of instructions to create more broad-coverage instructions that define (often new) tasks. Given the newly-generated set of instructions, the framework also creates input-output instances for them, which can be later used for supervising the instruction tuning. Finally, various heuristics are used to automatically filter low-quality or repeated instructions, before adding the remaining valid tasks to the task pool. This process can be repeated for many iterations until reaching a large number of tasks. We use the iterative SELF-INSTRUCT process on GPT-3 to generate about 52k instructions, paired with about 82K instance inputs and target outputs. On this resulting data, we build GPT3-Instruct by finetuning GPT-3 (i.e., the same model used for generating the instruction data).",
		"Experiment Plan": "We evaluate GPT3-Instruct in comparison to various other models on both typical NLP tasks included in SUPERNI, and a set of new instructions that are created for novel usage of instruction-following models."
	}
}

