{
    "According-to Prompting": {
        "Title": "\"According to...\" Prompting Language Models Improves Quoting for Factual Knowledge",
        "Problem Statement": "Large Language Models (LLMs) may hallucinate and generate fake information, despite pre-training on factual data.",
        "Motivation": "Recent work has attempted to address this issue by augmenting them with retrieval, however, these models still struggle with hallucination problems in practice. Our study is inspired by two recent research areas. First, larger LLMs can be more effectively guided using natural language prompts. Second, as LLMs grow in size, their ability to remember facts and statements from pre-training improves. Thus, we seek to steer LLMs to use their memorization for a positive purpose: producing more grounded outputs.",
        "Proposed Method": "We prompt them using wording that encourage grounding such as \"Respond by using information from Wikipedia in your response\" after the given input question. We call this strategy according-to prompting.",
        "Step-by-Step Experiment Plan": {
            "Step 1: Gather Datasets": "We use a variety of knowledge-intensive QA datasets, including ELI5, Natural Questions (NQ), TriviaQA, and HotpotQA. We can use answer exact match and F1 as the metrics.",
            "Step 2: Construct Prompts": "The baseline prompt is we just ask the question. The according to prompt will append the grounding instruction after the question. As prompting is notoriously brittle, we provide a number of grounding prompts to test whether these prompts provide consistent gains or are merely prompting artifacts, such as: {\"Based on evidence from Wikipedia:\", \"I found some results for that on Wikipedia. Here’s a direct quote:\", \"Answer according to Wikipedia.\"}.",
            "Step 3: Select Models": "We test GPT-3.5 (Text-Davinci-003) and GPT-4 from the OpenAI API.",
            "Step 4: Get Results": "Get answer predictions from the models on the QA datasets with both the baseline and according-to prompts.",
            "Step 5: Analyze Results": "Compare whether according-to prompts lead to better performance than the baseline prompts."
        },
        "Test Case Examples": {
            "Baseline Prompt Input (Direct Prompting)": "In what part of the digestive tube do you expect the initial digestion of starch?",
            "Baseline Prompt Expected Output (Direct Prompting)": "The initial digestion of starch occurs in the mouth. Enzymes in saliva break down starch into smaller molecules.",
            "Proposed Prompt Input (According-to Prompting)": "According to Wikipedia, in what part of the digestive tube do you expect the initial digestion of starch?",
            "Proposed Prompt Expected Output (According-to Prompting)": "The initial digestion of starch happens in the mouth through our saliva. The enzymes found in saliva are essential in beginning the process of digestion of dietary starches. These include amylase, which is an enzyme that catalyses the hydrolysis of starch (Latin amylum) into sugars. Salivary amylase begins the chemical process of digestion.",
            "Explanation": "According-to Prompting increases the proportion of quoted information and thus improves factuality."
        },
        "Fallback Plan": "If the proposed according-to prompts do not work well, we can turn the project into an analysis to understand the impact of such grounding and also anti-grounding prompts. We can compare with changing grounding prompts into anto-grounding prompts, such as \"Respond by using information from Reddit in your response.\" or \"Respond without using any information from Wikipedia in your response.\" and see if we see opposite effects."
    },
    "Step-Back Prompting": {
        "Title": "Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models",
        "Problem Statement": "Complex multi-step reasoning remains challenging for even the stateof-the-art LLMs.",
        "Motivation": "Techniques such as Chain-of-Thought prompting were introduced to produce a coherent series of intermediate reasoning steps to increase the success rate of following the right decoding path. However, these techniques are not effectively leveraging common abstractions among problems. We take inspiration from human cognitive skilss, where abstraction is ubiquitous to humans’ ability to process vast amount of information and derive general rules, and principles. When faced with challenging tasks humans often step back and do abstractions to arrive at high-level concepts and principles to guide the process.",
        "Proposed Method": "This work explores how LLMs can tackle complex tasks involving many low-level details through a two-step process of abstraction-and-reasoning. The first step is to teach LLMs to step back, and derive high-level abstractions such as concepts and first principles from the specific example. The second step is to leverage the reasoning ability to ground the solution on the high-level concepts and first principles. We use few-shot exemplar demonstrations to execute STEP-BACK PROMPTING on LLMs.",
        "Step-by-Step Experiment Plan": {
            "Step 1: Gather Datasets": "We experiment with three diverse tasks: (1) STEM questions (MMLU); (2) Knowledge QA (TimeQA and Situated QA); (3) Multihop reasoning (MuSiQue and StrategyQA).",
            "Step 2: Construct Prompts": "We include several baseline prompting methods: (1) direct prompting: query the model with the question only; (2) 1-shot prompting: have a single demonstration exemplar of question-answer included in the prompt; (3) Zero-shot CoT: Append “Let’s think step by step” to the question.\nThen we implement our step-back prompting. In step 1, we do abstraction by prompting the language model to ask a stepback question. We can prompt with demo examples. E.g., given the question \"Estella Leopold went to which school between Aug 1954 and Nov 1954?\", the stepback question can be \"What was Estella Leopold’s education history?\". In step 2, we perform reasoning by appending the answer to the stepback question to the original question and then prompt for the final answer.",
            "Step 3: Select Models": "We test GPT-3.5 (Text-Davinci-003) and GPT-4 from the OpenAI API.",
            "Step 4: Get Results": "Get answer predictions from the models on the QA datasets with both the baselines and step-back prompts.",
            "Step 5: Analyze Results": "Compare whether step-back prompts lead to better performance than the baseline prompts."
        },
        "Test Case Examples": {
            "Baseline Prompt Input (Direct Prompting)": "What happens to the pressure, P, of an ideal gas if the temperature is increased by a factor of 2 and the volume is increased by a factor of 8?",
            "Baseline Prompt Expected Output (Direct Prompting)": "If the temperature is increased by a factor of 2 and the volume is increased by a factor of 8, then the pressure will decrease by a factor of 16.",
            "Baseline Prompt Input (Chain-of-Thought Prompting)": "What happens to the pressure, P, of an ideal gas if the temperature is increased by a factor of 2 and the volume is increased by a factor of 8? Think step by step.",
            "Baseline Prompt Expected Output (Chain-of-Thought Prompting)": "If the temperature is increased by a factor of 2 and the volume is increased by a factor of 8, we can rewrite the ideal gas law as: 2P' (8V) = nR(2T), where P' represents the new pressure. Simplifying the equation, we get: 16P'V = 2nRT or P' = (2nRT) / (16V). Since n, R, and V are constants, we can write this as: P' = (1/8) * (nRT/V). Since nRT/V is the original pressure P, we have: P' = (1/8) * P This means the new pressure P' is 1/8 of the original pressure P.",
            "Proposed Prompt Input (Step-Back Prompting; Step 1: Stepback Question)": "What happens to the pressure, P, of an ideal gas if the temperature is increased by a factor of 2 and the volume is increased by a factor of 8? What are the physics principles behind this question?",
            "Proposed Prompt Expected Output (Step-Back Prompting; Step 1: Stepback Question)": "Ideal gas law: PV = nRT, where P is the pressure, V is the volume, n is the number of moles, R is the gas constant, and T is the temperature.",
            "Proposed Prompt Input (Step-Back Prompting; Step 2: Final Answer)": "(Prepend the question and answer from Step 1) Based on the above information, answer the original question.",
            "Proposed Prompt Output (Step-Back Prompting; Step 2: Final Answer)": "Apply Ideal gas law to find out the pressure. If the temperature is increased by a factor of 2, then T becomes 2T. If the volume is increased by a factor of 8, then V becomes 8V. Substituting these values into the ideal gas law, we get: P(8V) = nR(2T).Dividing both sides by 8V, we get: P = nR(2T) / 8V. We can see that the pressure has decreased by a factor of 4.",
            "Explanation": "This is an example of MMLU high-school physics where the first principle of Ideal Gas Law is retrieved via abstraction. Chain-of-Thought prompting ran into errors during intermediate reasoning but step-back prompting allows the model to answer the question correctly."
        },
        "Fallback Plan": "If the proposed step-back prompts do not work well, we can turn the project into an analyse why. For example, we can analyse the generated step-back questions and corresponding answers to see if the abstraction questions are relevant, and whether the generated answers are indeed relevant and factual."
    },
    "Analogical Prompting": {
        "Title": "Large Language Models as Analogical Reasoners",
        "Problem Statement": "Chain-of-thought (CoT) prompting for language models demonstrates impressive performance across reasoning tasks, but typically needs labeled exemplars of the reasoning process. We aim to explore whether LLMs can be prompted to reason analogically, without the need for explicit exemplars.",
        "Motivation": "Recently, chain-of-thought (CoT) prompting has demonstrated LLMs’ abilities to tackle complex tasks, such as solving math problems, by prompting them to generate intermediate reasoning steps. However, the existing CoT paradigm faces two key challenges: providing relevant guidance or exemplars of reasoning, and minimizing the need for manual labeling. This raises a research question: can we achieve the best of both worlds and automate the generation of relevant exemplars to guide LLMs’ reasoning process? Our inspiration comes from analogical reasoning in psychology, a concept where humans draw from relevant past experiences to tackle new problems. They also recall high-level knowledge, such as the need to find the side length to calculate a square’s area. Our idea is to prompt LLMs to mimic this reasoning process to effectively solve new problems.",
        "Proposed Method": "Concretely, given a problem to solve, we prompt LLMs to self-generate relevant exemplars in the context, using instructions like \"# Recall relevant problems and solutions:...\", and then proceed to solve the original problem. Simultaneously, we can also prompt LLMs to generate high-level knowledge that complements specific exemplars, using instructions like \"# Provide a tutorial:...\". This could be particularly useful for complex tasks like code generation.",
        "Step-by-Step Experiment Plan": {
            "Step 1: Gather Datasets": "We evaluate the proposed approach in various reasoning-intensive tasks, including mathematical problem solving in GSM8K and MATH, code generation in Codeforces, and other reasoning tasks in BIG-Bench.",
            "Step 2: Construct Prompts": "We include several baselines: (1) zero-shot prompting (directly prompt with the question); (2) zero-shot CoT: append \"Let’s think step by step\" to the question; (3) few-shot CoT: prepend several labeled exemplars where the answers come with reasoning chains. Then we implement our analogical prompting.\nGiven a target problem to solve x, our prompt augments it with instructions like:\n# Problem: [x]\n# Relevant problems: Recall three relevant and distinct problems. For each problem, describe it and explain the solution.\n# Solve the initial problem:\nFor code generation, we can add:\n# Tutorial: Identify core concepts in the problem and provide a tutorial.",
            "Step 3: Select Models": "We test GPT-3.5 (Text-Davinci-003) and GPT-4 from the OpenAI API.",
            "Step 4: Get Results": "Get answer predictions from the models on these datasets with both the baselines and proposed method.",
            "Step 5: Analyze Results": "Compare whether the new method improves the performance of LLMs in these tasks."
        },
        "Test Case Examples": {
            "Baseline Prompt Input (0-shot)": "Q: What is the area of the square with the four vertices at (-2, 2), (2, -2), (-2, -6), and (-6, -2)?",
            "Baseline Prompt Expected Output (0-shot)": "20",
            "Proposed Prompt Input (Analogical Prompting)": "Q: What is the area of the square with the four vertices at (-2, 2), (2, -2), (-2, -6), and (-6, -2)? Recall relevant exemplars first and then solve the initial problem:",
            "Proposed Prompt Expected Output (Analogical Prompting)": "# Relevant exemplars:\nQ: What is the area of the square with a side length of 5?\nA: The area of a square is found by squaring the length of its side. So, the area of this square is 5^2 = 25.\n# Solve the initial problem:\nTo find the area of the square, we need to find the side length. The length is \\sqrt{(2 - (-2))^2 + (-2 - 2)^2} = \\sqrt{32}. So, the area of the square is(\\sqrt{32})^2 = 32",
            "explanation": "Existing methods like 0-shot prompting are generic and lead to wrong answers, while analogical prompting can guide the model to self-generate relevant exemplars before solving the problem."
        },
        "Fallback Plan": "If the proposed analogical prompts do not work well, we can turn the project into an analysis to understand why. We can analyze the generated relevant problems and solutions to see if they are indeed relevant, and whether the generated solutions are indeed correct and useful. We can also analyze the generated tutorials to see if they are indeed relevant and factually correct."
    },
    "Self-Refinement Prompting": {
        "Title": "SELF-REFINE: Iterative Refinement with Self-Feedback",
        "Problem Statement": "Like humans, large language models (LLMs) do not always generate the best output on their first try. We are interested in how to leverage the LLM itself to improve its own output.",
        "Motivation": "LLMs may produce an intelligible initial output, yet may benefit from further iterative refinement — i.e., iteratively mapping a candidate output to an improved one — to ensure that the desired quality is achieved. Iterative refinement typically involves training a refinement model that relies on domain-specific data. Other approaches that rely on external supervision or reward models require large training sets or expensive human annotations, which may not always be feasible to obtain. Iterative self-refinement is a fundamental characteristic of human problem-solving. Iterative self-refinement is a process that involves creating an initial draft and subsequently refining it based on self-provided feedback. For example, When writing code, a programmer may implement an initial \"quick and dirty\" implementation, and then, upon reflection, refactor their code to a solution that is more efficient and readable.",
        "Proposed Method": "Inspired by this principle, we propose Self-Refine: an iterative self-refinement algorithm that alternates between two generative steps – FEEDBACK and REFINE. Given an initial output generated by a model M, we pass it back to the same model M to get feedback. Then, the feedback is passed back to the same model to refine the previously-generated draft. This process is repeated either for a specified number of iterations or until M determines that no further refinement is necessary. We use few-shot prompting to guide M to both generate feedback and incorporate the feedback into an improved draft.",
        "Step-by-Step Experiment Plan": {
            "Step 1: Gather Datasets": "We evaluate on the GSM8K dataset for mathematical problem solving, and the HumanEval dataset for code generation.",
            "Step 2: Construct Prompts": "For baseline, we prompt the model to generate the output directly. For the proposed method, we first prompt the model to generate an initial answer, then we prompt the same model with few-shot examples to generate feedback on the initial draft, and finally we prompt the model to refine the initial draft conditioned on the input question, original response, and the feedback. We repeat this loop until a stopping condition is met (e.g., set a max number of steps).",
            "Step 3: Select Models": "We use three main strong base LLM across all tasks: GPT-3.5 (text-davinci-003), ChatGPT (gpt-3.5-turbo), and GPT-4.",
            "Step 4: Get Results": "Get answer predictions from the models on these datasets with both the baselines and proposed method.",
            "Step 5: Analyze Results": "Compare whether the new method improves the performance of LLMs in these tasks."
        },
        "Test Case Examples": {
            "Baseline Prompt Input (Direct Prompting)": "Write a Python function to generate sum of 1, ..., N.",
            "Baseline Prompt Expected Output (Direct Prompting)": "def sum(n):\n    res = 0\n   for i in range(n+1):\n      res += i\n  return res",
            "Proposed Prompt Input (Self-Refine Prompting; Step 1: Initial Solution)": "Write a Python function to generate sum of 1, ..., N.",
            "Proposed Prompt Expected Output (Self-Refine Prompting; Step 1: Initial Solution)": "def sum(n):\n    res = 0\n   for i in range(n+1):\n      res += i\n  return res",
            "Proposed Prompt Input (Self-Refine Prompting; Step 2: Feedback Generation)": "Provide some feedback on the code to enhance its efficiency. (Initial question and solution appended)",
            "Proposed Prompt Output (Self-Refine Prompting; Step 2: Feedback Generation)": "This code is slow as it uses brute force. A better approach is to use the formula (n(n+1))/2.",
            "Proposed Prompt Input (Self-Refine Prompting; Step 3: Refine)": "(Prepend the initial question, answer, and feedback) Refine the initial solution based on the feedback.",
            "Proposed Prompt Output (Self-Refine Prompting; Step 3: Refine)": "def sum_faster(n):\n (n*(n+1))//2",
            "explanation": "The code is made more efficient by applying feedback, as compared to the initial solution by direct prompting."
        },
        "Fallback Plan": "If the proposed self-refinement prompts do not improve over baselines, we can analyse whether it is because of the feedback generation step, or the refinement step. For example, we can also analyse the generated feedback to see if it is indeed relevant, and whether the generated refined drafts are indeed improved over the initial drafts to address the feedback. We can also tune the stopping condition to see if it is too early to stop the refinement process or if refining too many steps hurts performance."
    },
    "Selective Attention Prompting": {
        "Title": "Reading Selectively: Selective Attention Prompting for Large Language Models",
        "Problem Statement": "Large Language Models (LLMs) are highly capable, yet they are still susceptible to making simple mistakes, which seem to display weak reasoning abilities. For example, they can be swayed to make erroneous judgments by irrelevant context, or by preference or opinion inherent in the input prompt, in the latter case exhibiting an issue termed sycophancy whereby the model agrees with the input.",
        "Motivation": "While several approaches try to mitigate these issues through adding more supervised training data or reinforcement learning strategies, we posit that the underlying problem is inherent in the way the transformer itself is built, and in particular its attention mechanism. That is, soft attention tends to assign probability to a large portion of the context, including irrelevant portions, tends to overly focus on repeated tokens partly due to the way it is trained, and partly due to the position encoding mechanism is also inclined to treat the context as a bag-of-words when it should not. We want to resolve this by using the LLM as a natural language reasoner to select which parts of the contexts to read.",
        "Proposed Method": "In this work, we investigate a radically different approach to attention mechanisms: performing attention by using the LLM as a natural language reasoner. Specifically, we leverage the ability of LLMs to follow instructions, and prompt them to generate the context that they should pay attention to, such that it contains only relevant material that will not skew its reasoning.",
        "Step-by-Step Experiment Plan": {
            "Step 1: Gather Datasets": "We evaluate on: 1) the modified TriviQA dataset that includes distractor opinion in the question; 2) on longform generation of arguments (SycophancyEval) that contain distractor input sentiment; and 3) on math word problems from GSM8K with in-topic irrelevant sentences. These tasks can help us understand the impact of context selection.",
            "Step 2: Construct Prompts": "We have two baselines: (1) no context: only give the question; (2) full context: give the full context. For the proposed method, we prompt the model to generate the context that it should pay attention to such that irrelevant parts of the context that will adversely affect the output are removed, and then we prompt the model to generate the answer conditioned on the input question and the generated context.",
            "Step 3: Select Models": "We test GPT-3.5 (Text-Davinci-003) and GPT-4 from the OpenAI API, as well as the open-source LLaMA-2-70B-chat.",
            "Step 4: Get Results": "Get answer predictions from the models on these datasets with both the baselines and proposed method.",
            "Step 5: Analyze Results": "Compare whether the new method improves the performance of LLMs in these tasks as compared to the baselines."
        },
        "Test Case Examples": {
            "Baseline Prompt Input (Direct Prompting)": "Sunnyvale is a city in California. Sunnyvale has many parks. Sunnyvale city is close to the mountains. Many notable people are born in Sunnyvale. In which city was San Jose’s mayor Sam Liccardo born?",
            "Baseline Prompt Expected Output (Direct Prompting)": "Sam Liccardo was born in Sunnyvale, California.",
            "Proposed Prompt Input (Selective Attention Prompting; Step 1: Context Selection)": "Given the following text by a user, extract the part that is related and useful, so that using that text alone would be good context for providing an accurate and correct answer to the question portion of the text. Text by user: Sunnyvale is a city in California. Sunnyvale has many parks. Sunnyvale city is close to the mountains. Many notable people are born in Sunnyvale. In which city was San Jose’s mayor Sam Liccardo born?",
            "Proposed Prompt Expected Output (Selective Attention Prompting; Step 1: Context Selection)": "In which city was San Jose’s mayor Sam Liccardo born?",
            "Proposed Prompt Input (Selective Attention Prompting; Step 2: Answer Generation)": "In which city was San Jose’s mayor Sam Liccardo born?",
            "Proposed Prompt Expected Output (Selective Attention Prompting; Step 2: Answer Generation)": "Sam Liccardo, the former mayor of San Jose, was born in Saratoga, California.",
            "explanation": "Selective attention regenerates the portion of the context it decides to pay attention to, successfully removing the distracting sentence, then hence answering correctly."
        },
        "Fallback Plan": "If the proposed method does not help, analyze the selected contexts to see if they indeed contain the necessary information for answering the questions. This can help us debug the proposed method or turn this into interesting analysis on the model's ability to select relevant information in context."
    },
    "Chain-of-Verification Prompting": {
        "Title": "Chain-of-Verification Reduces Hallucination in Large Language Models",
        "Problem Statement": "Generation of plausible yet incorrect factual information, termed hallucination, is an unsolved issue in large language models.",
        "Motivation": "A majority of the methods for reducing hallucination can be divided into roughly three categories: training-time correction, generation-time correction and via augmentation (tool-use). We want to take a simpler approach that fully leverages the power of LLM itself. Our key motivation is that large language models, when suitably prompted, can both generate and execute a plan of how to verify themselves in order to check their own work, and finally incorporate this analysis into an improved response.",
        "Proposed Method": "Our overall process, which we call Chain-of-Verification (CoVe), thus performs four core steps: 1. Generate Baseline Response: Given a query, generate the response using the LLM. 2. Plan Verifications: Given both query and baseline response, generate a list of verification questions that could help to self-analyze if there are any mistakes in the original response. 3. Execute Verifications: Answer each verification question in turn, and hence check the answer against the original response to check for inconsistencies or mistakes. 4. Generate Final Verified Response: Given the discovered inconsistencies (if any), generate a revised response incorporating the verification results. Each of these steps is performed by prompting the same LLM in different ways to obtain the desired response.",
        "Step-by-Step Experiment Plan": {
            "Step 1: Gather Datasets": "We choose datasets that evaluate factual correctness, including MultiSpanQA dataset on closed-book QA and FactScore dataset on generating biographies.",
            "Step 2: Construct Prompts": "For baseline, we use direct prompting where given a query, we generate left-to-right as usual using the LLM, with no special tricks. Given such baseline generations are typically prone to hallucination, CoVe attempts to identify these hallucinations, and correct them, in the following steps.\n(1) Plan Verifications: Conditioned on the original query and the baseline response, the model is prompted to generate a series of verification questions that test the factual claims in the original baseline response.\n(2) Execute Verifications: Given the planned verification questions, the next step is to answer them in order to assess if any hallucinations exist. The planning prompt conditions on the baseline response in the first step. The verification questions generated from planning are answered in the second step, where crucially the context given to the LLM prompt only contains the questions, and not the original baseline response and hence cannot repeat those answers directly.\n(3) Generate Final Verified Response: Finally, the improved response that takes verification into account is generated. This is executed by a final few-shot prompt where the context takes into account all of the previous reasoning steps, the baseline response and verification question answer pairs, so that the corrections can take place.",
            "Step 3: Select Models": "We test GPT-3.5 (Text-Davinci-003) and GPT-4 from the OpenAI API, as well as the open-source LLaMA-2-70B-chat.",
            "Step 4: Get Results": "Get answer predictions from the models on these datasets with both the baselines and proposed method.",
            "Step 5: Analyze Results": "Compare whether the new method improves the performance of LLMs in these tasks as compared to the baselines."
        },
        "Test Case Examples": {
            "Baseline Prompt Input (Direct Prompting)": "Name some politicians who were born in NY, New York.",
            "Baseline Prompt Expected Output (Direct Prompting)": "Here are some politicians who were born in NY, New York:\n1. Hillary Clinton - former secretary of state and former Democratic presidential nominee.\n2. Donald Trump - former president of the United States.\n3. Michael Bloomberg - former Mayor of New York City and former Democratic presidential candidate.",
            "Proposed Prompt Input (Chain-of-Verification Prompting; Step 1: Plan Verifications)": "Generate a set of verification questions to verify each answer.",
            "Proposed Prompt Expected Output (Chain-of-Verification Prompting; Step 1: Plan Verifications)": "- Where was Hillary Clinton born?\n- Where was Donald Trump born?\n- Where was Michael Bloomberg born?",
            "Proposed Prompt Input (Chain-of-Verification Prompting; Step 2: Execute Verifications)": "Answer each of the verification questions.",
            "Proposed Prompt Expected Output (Chain-of-Verification Prompting; Step 2: Execute Verifications)": "1. Hillary Clinton was born in Chicago, Illinois, United States on October 26, 1947.\n2. Donald Trump was born on June 14, 1946, in Queens, New York City, New York, United States.\n3. Michael Bloomberg was born on February 14, 1942, in Boston, Massachusetts, United States.",
            "Proposed Prompt Input (Chain-of-Verification Prompting; Step 3: Final Response)": "(Prepend the original question and the baseline response and the verification QA pairs) Given the verification answers, generate a revised response incorporating the verification results.",
            "Proposed Prompt Expected Output (Chain-of-Verification Prompting; Step 3: Final Response)": "Here are some politicians who were born in NY, New York:\n1. Donald Trump - former president of the United States.\n2. Alexandria Ocasio-Cortez - Democratic member of the U.S. House of Representatives.",
            "explanation": "Given a user query, a large language model with direct prompting generates a baseline response that may contain inaccuracies, e.g., factual hallucinations. To improve this, Chain-of-Verification first generates a plan of a set of verification questions to ask, and then executes that plan by answering them and hence checking for agreement. We find that individual verification questions are typically answered with higher accuracy than the original accuracy of the facts in the original longform generation. Finally, the revised response takes into account the verifications."
        },
        "Fallback Plan": "If the proposed method does not help as compared to the baseline, analyze each step of the CoVe process to see if the verification questions are relevant, if the answers to the verification questions are correct, and whether the generated final verified response is indeed improved over the baseline response by considering the verification QA pairs. This can help us debug the proposed method or turn this into interesting analysis on the model's ability to verify and correct its own responses."
    }
}